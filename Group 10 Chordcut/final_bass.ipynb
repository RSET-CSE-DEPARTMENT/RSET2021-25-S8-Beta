{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzTcj72RUuGH",
        "outputId": "499bd559-8254-41f4-db13-c46910c3f116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# root_dir = \"/content/drive/MyDrive/project\"\n",
        "# import os\n",
        "# os.chdir(root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45v77ow7cyNz",
        "outputId": "1facaa1b-3a80-4d57-b6c8-da99b8c59a77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement tensorflow==2.14.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.19.0rc0)\n",
            "ERROR: No matching distribution found for tensorflow==2.14.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting librosa==0.9.2\n",
            "  Using cached librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa==0.9.2)\n",
            "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from librosa==0.9.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from librosa==0.9.2) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from librosa==0.9.2) (1.5.1)\n",
            "Requirement already satisfied: joblib>=0.14 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from librosa==0.9.2) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from librosa==0.9.2) (5.1.1)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2)\n",
            "  Using cached resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numba>=0.45.1 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from librosa==0.9.2) (0.60.0)\n",
            "Collecting soundfile>=0.10.2 (from librosa==0.9.2)\n",
            "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
            "Collecting pooch>=1.0 (from librosa==0.9.2)\n",
            "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from librosa==0.9.2) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from numba>=0.45.1->librosa==0.9.2) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa==0.9.2) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa==0.9.2) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->librosa==0.9.2) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from soundfile>=0.10.2->librosa==0.9.2) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heinzkoshy\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.9.2) (2024.12.14)\n",
            "Using cached librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "Using cached resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\http\\client.py\", line 479, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\ssl.py\", line 1251, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\ssl.py\", line 1103, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
            "    status = _inner_run()\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
            "    return self.run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 379, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in _complete_partial_requirements\n",
            "    for link, (filepath, _) in batch_download:\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 184, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 55, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(value)\n",
            "  File \"C:\\Users\\Heinzkoshy\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.14.0\n",
        "#!pip install librosa==0.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ILB4JjwqUtl2"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wavfile\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, BatchNormalization, LeakyReLU, ReLU, Dropout, Concatenate, Multiply, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from scipy.signal import resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ9LTAMiUtl4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_audio_data(dataset_path, target_sampling_rate=16000):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for music_folder in tqdm(os.listdir(dataset_path), desc=\"Processing dataset\"):\n",
        "        music_path = os.path.join(dataset_path, music_folder)\n",
        "\n",
        "        if os.path.isdir(music_path):\n",
        "\n",
        "            mixture_path = os.path.join(music_path, \"mixture.wav\")\n",
        "            drums_path = os.path.join(music_path, \"bass.wav\")\n",
        "\n",
        "            if os.path.exists(mixture_path) and os.path.exists(drums_path):\n",
        "\n",
        "                sr_mix, mixture = wavfile.read(mixture_path)\n",
        "                sr_drums, drums = wavfile.read(drums_path)\n",
        "\n",
        "                if sr_mix != target_sampling_rate:\n",
        "                    mixture = resample(mixture, int(len(mixture) * target_sampling_rate / sr_mix))\n",
        "                    sr_mix = target_sampling_rate\n",
        "\n",
        "                if sr_drums != target_sampling_rate:\n",
        "                    drums = resample(drums, int(len(drums) * target_sampling_rate / sr_drums))\n",
        "                    sr_drums = target_sampling_rate\n",
        "\n",
        "                # Convert to mono if stereo\n",
        "                if len(mixture.shape) > 1:\n",
        "                    mixture = mixture.mean(axis=1)\n",
        "                if len(drums.shape) > 1:\n",
        "                    drums = drums.mean(axis=1)\n",
        "\n",
        "                mixture = mixture / np.max(np.abs(mixture))\n",
        "                drums = drums / np.max(np.abs(drums))\n",
        "\n",
        "                audio_chunks = [mixture[i:i+64] for i in range(0, len(mixture) - 64 + 1, 64)]\n",
        "                drums_chunks = [drums[i:i+64] for i in range(0, len(drums) - 64 + 1, 64)]\n",
        "\n",
        "                x_list.extend(audio_chunks)\n",
        "                y_list.extend(drums_chunks)\n",
        "\n",
        "                print(f\"Processing '{music_folder}': {len(audio_chunks)} chunks\")\n",
        "\n",
        "    return np.array(x_list), np.array(y_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR2ZWFP3Utl4"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = \"musdb18/train\"\n",
        "test_dataset_path = \"musdb18/valid\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGo7NgJMUtl5",
        "outputId": "9ad4399e-7c00-4b5b-8914-b70a44445e8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing dataset:   1%|          | 1/100 [00:22<37:38, 22.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Young Griffo - Pennies': 69498 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   2%|▏         | 2/100 [00:39<31:09, 19.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Young Griffo - Blood To Bone': 63645 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   3%|▎         | 3/100 [00:51<25:42, 15.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Young Griffo - Facade': 42013 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   4%|▍         | 4/100 [01:07<25:35, 15.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Wall Of Death - Femme': 59779 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   5%|▌         | 5/100 [02:14<54:21, 34.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Traffic Experiment - Sirens': 105364 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   6%|▌         | 6/100 [02:35<46:55, 29.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Scarlet Brand - Les Fleurs Du Mal': 75909 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   7%|▋         | 7/100 [02:49<38:16, 24.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Wrong'Uns - Rothko': 50584 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   8%|▊         | 8/100 [03:44<52:42, 34.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Traffic Experiment - Once More (With Feeling)': 108815 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:   9%|▉         | 9/100 [04:04<45:02, 29.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Voelund - Comfort Lives In Belief': 52523 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  10%|█         | 10/100 [04:09<33:03, 22.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The So So Glos - Emergency': 41750 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  11%|█         | 11/100 [04:28<31:37, 21.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Titanium - Haunted Age': 62072 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  12%|█▏        | 12/100 [04:40<26:51, 18.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Triviul - Dorothy': 46889 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  13%|█▎        | 13/100 [04:56<25:27, 17.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Triviul - Angelsaint': 59224 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  14%|█▍        | 14/100 [05:13<25:16, 17.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Tim Taler - Stalker': 59454 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  15%|█▌        | 15/100 [05:20<20:11, 14.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Strand Of Oaks - Spacestation': 60965 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  16%|█▌        | 16/100 [05:38<21:43, 15.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Long Wait - Back Home To Blue': 65162 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  17%|█▋        | 17/100 [05:57<22:49, 16.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Skelpolu - Together Alone': 81500 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  18%|█▊        | 18/100 [06:16<23:33, 17.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'St Vitus - Word Gets Around': 61802 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  19%|█▉        | 19/100 [07:08<37:17, 27.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Districts - Vermont': 57038 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  20%|██        | 20/100 [07:27<33:17, 24.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Spike Mullings - Mike's Sulking': 64218 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  21%|██        | 21/100 [07:39<28:02, 21.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Swinging Steaks - Lost My Way': 77536 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  22%|██▏       | 22/100 [07:53<24:40, 18.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Sweet Lights - You Let Me Down': 97995 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  23%|██▎       | 23/100 [08:16<25:47, 20.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Steven Clark - Bounty': 72365 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  24%|██▍       | 24/100 [08:33<24:26, 19.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Snowmine - Curfews': 68798 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  25%|██▌       | 25/100 [08:56<25:21, 20.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Port St Willow - Stay Even': 79253 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  26%|██▌       | 26/100 [09:10<22:43, 18.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Phre The Eon - Everybody's Falling Apart': 56104 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  27%|██▋       | 27/100 [09:23<20:37, 16.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Night Panther - Fire': 53249 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  28%|██▊       | 28/100 [09:43<21:25, 17.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Secret Mountains - High Horse': 88875 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  29%|██▉       | 29/100 [10:12<24:53, 21.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Skelpolu - Human Mistakes': 81171 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  30%|███       | 30/100 [10:54<32:05, 27.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Remember December - C U Next Time': 60678 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  31%|███       | 31/100 [11:15<29:26, 25.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Patrick Talbot - A Reason To Leave': 64936 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  32%|███▏      | 32/100 [11:34<26:38, 23.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Patrick Talbot - Set Me Free': 72495 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  33%|███▎      | 33/100 [11:49<23:18, 20.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'North To Alaska - All The Same': 62035 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  34%|███▍      | 34/100 [11:51<16:44, 15.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Rockabilly': 6487 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  35%|███▌      | 35/100 [11:52<12:03, 11.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Punk': 7190 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  36%|███▌      | 36/100 [11:54<08:45,  8.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Hendrix': 4960 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  37%|███▋      | 37/100 [11:55<06:21,  6.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Rock': 3272 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  38%|███▊      | 38/100 [11:56<04:40,  4.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Country2': 4363 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  39%|███▉      | 39/100 [11:59<04:12,  4.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Britpop': 9197 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  40%|████      | 40/100 [12:00<03:21,  3.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Reggae': 4365 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  41%|████      | 41/100 [12:02<02:46,  2.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Country1': 8685 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  42%|████▏     | 42/100 [12:05<02:50,  2.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Grunge': 10463 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  43%|████▎     | 43/100 [12:10<03:24,  3.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Gospel': 18934 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  44%|████▍     | 44/100 [12:16<03:49,  4.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Disco': 31194 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  45%|████▌     | 45/100 [12:32<07:01,  7.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Leaf - Come Around': 66145 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  46%|████▌     | 46/100 [12:36<05:58,  6.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Leaf - Summerghost': 58000 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  47%|████▋     | 47/100 [13:03<11:24, 12.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Meaxic - You Listen': 103178 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  48%|████▊     | 48/100 [13:40<17:25, 20.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Lushlife - Toynbee Suite': 157139 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  49%|████▉     | 49/100 [14:14<20:33, 24.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Meaxic - Take A Step': 70678 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  50%|█████     | 50/100 [14:15<14:27, 17.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - 80s Rock': 9229 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  51%|█████     | 51/100 [14:23<11:53, 14.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Matthew Entwistle - Dont You Ever': 28504 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  52%|█████▏    | 52/100 [14:31<09:52, 12.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Leaf - Wicked': 47705 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  53%|█████▎    | 53/100 [14:43<09:36, 12.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Jokers, Jacks & Kings - Sea Of Leaves': 47912 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  54%|█████▍    | 54/100 [14:45<07:08,  9.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Music Delta - Beatles': 9093 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  55%|█████▌    | 55/100 [14:53<06:45,  9.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'James May - Dont Let Go': 60535 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  56%|█████▌    | 56/100 [15:12<08:36, 11.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Hop Along - Sister Cities': 70855 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  57%|█████▋    | 57/100 [15:26<08:58, 12.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'James May - All Souls Moon': 55261 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  58%|█████▊    | 58/100 [15:36<08:19, 11.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Hollow Ground - Left Blind': 39822 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  59%|█████▉    | 59/100 [15:42<06:48,  9.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Johnny Lokke - Promises & Lies': 71500 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  60%|██████    | 60/100 [15:59<08:01, 12.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'James May - On The Line': 64068 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  61%|██████    | 61/100 [16:05<06:40, 10.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'James May - If You Say': 64625 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  62%|██████▏   | 62/100 [16:21<07:38, 12.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Jay Menon - Through My Eyes': 63339 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  63%|██████▎   | 63/100 [16:41<08:59, 14.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Johnny Lokke - Whisper To A Scream': 63879 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  64%|██████▍   | 64/100 [16:57<08:54, 14.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Invisible Familiars - Disturbing Wildlife': 54673 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  65%|██████▌   | 65/100 [17:16<09:18, 15.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Faces On Film - Waiting For Ga': 64408 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  66%|██████▌   | 66/100 [17:29<08:40, 15.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Giselle - Moss': 50472 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  67%|██████▋   | 67/100 [18:01<11:11, 20.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Drumtracks - Ghost Bitch': 89273 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  68%|██████▊   | 68/100 [18:14<09:34, 17.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Fergessen - Nos Palpitants': 49602 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  69%|██████▉   | 69/100 [18:27<08:31, 16.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Grants - PunchDrunk': 51150 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  70%|███████   | 70/100 [18:47<08:48, 17.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Hezekiah Jones - Borrowed Heart': 60394 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  71%|███████   | 71/100 [18:51<06:27, 13.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Fergessen - The Wind': 48000 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  72%|███████▏  | 72/100 [19:27<09:30, 20.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Flags - 54': 78835 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  73%|███████▎  | 73/100 [19:38<07:49, 17.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Fergessen - Back From The Start': 42186 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  74%|███████▍  | 74/100 [19:45<06:11, 14.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Helado Negro - Mitad Del Mundo': 45462 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  75%|███████▌  | 75/100 [19:57<05:38, 13.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Clara Berry And Wooldog - Waltz For My Victims': 43857 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  76%|███████▌  | 76/100 [20:06<04:52, 12.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Chris Durban - Celebrate': 75446 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  77%|███████▋  | 77/100 [20:28<05:52, 15.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Clara Berry And Wooldog - Stella': 48935 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  78%|███████▊  | 78/100 [20:53<06:36, 18.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Black Bloc - If You Want Success': 99683 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  79%|███████▉  | 79/100 [21:13<06:33, 18.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Cnoc An Tursa - Bannockburn': 73675 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  80%|████████  | 80/100 [21:33<06:22, 19.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Dark Ride - Burning Bridges': 58212 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  81%|████████  | 81/100 [21:51<05:59, 18.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Celestial Shore - Die For Us': 69668 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  82%|████████▏ | 82/100 [22:03<04:59, 16.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Creepoid - OldTree': 75551 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  83%|████████▎ | 83/100 [22:14<04:14, 14.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Clara Berry And Wooldog - Air Traffic': 43361 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  84%|████████▍ | 84/100 [22:36<04:32, 17.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Dreamers Of The Ghetto - Heavy Love': 73746 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  85%|████████▌ | 85/100 [22:51<04:08, 16.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'ANiMAL - Clinic A': 59512 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  86%|████████▌ | 86/100 [23:00<03:19, 14.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Auctioneer - Our Future Faces': 51970 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  87%|████████▋ | 87/100 [23:23<03:39, 16.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Alexander Ross - Velvet Curtain': 128622 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  88%|████████▊ | 88/100 [23:33<02:59, 14.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'ANiMAL - Rockshow': 41424 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  89%|████████▉ | 89/100 [23:48<02:43, 14.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Bill Chudziak - Children Of No-one': 57733 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  90%|█████████ | 90/100 [24:02<02:25, 14.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'ANiMAL - Easy Tiger': 51414 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  91%|█████████ | 91/100 [24:07<01:46, 11.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'BigTroubles - Phantom': 36733 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  92%|█████████▏| 92/100 [24:21<01:39, 12.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Angela Thomas Wade - Milk Cow Blues': 52772 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  93%|█████████▎| 93/100 [24:38<01:35, 13.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'AvaLuna - Waterduct': 64826 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  94%|█████████▍| 94/100 [24:55<01:27, 14.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Atlantis Bound - It Was My Fault For Waiting': 67059 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  95%|█████████▌| 95/100 [25:06<01:08, 13.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Aimee Norwich - Child': 47317 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  96%|█████████▌| 96/100 [25:20<00:54, 13.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Actions - South Of The Water': 44200 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  97%|█████████▋| 97/100 [25:31<00:38, 12.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Actions - One Minute Smile': 40893 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  98%|█████████▊| 98/100 [25:43<00:25, 12.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'A Classic Education - NightOwl': 42860 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  99%|█████████▉| 99/100 [26:11<00:17, 17.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Alexander Ross - Goodbye Bolero': 104704 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing dataset: 100%|██████████| 100/100 [26:25<00:00, 15.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Actions - Devil's Words': 49201 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing dataset:   6%|▋         | 1/16 [00:09<02:19,  9.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Young Griffo - Facade': 42013 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  12%|█▎        | 2/16 [00:24<02:57, 12.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Young Griffo - Pennies': 69498 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  19%|█▉        | 3/16 [00:42<03:20, 15.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Titanium - Haunted Age': 62072 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  25%|██▌       | 4/16 [01:36<06:06, 30.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Traffic Experiment - Once More (With Feeling)': 108815 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  31%|███▏      | 5/16 [01:55<04:48, 26.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Voelund - Comfort Lives In Belief': 52523 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  38%|███▊      | 6/16 [02:06<03:29, 20.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Triviul - Dorothy': 46889 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  44%|████▍     | 7/16 [02:21<02:51, 19.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Triviul - Angelsaint': 59224 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  50%|█████     | 8/16 [03:26<04:29, 33.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Traffic Experiment - Sirens': 105364 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  56%|█████▋    | 9/16 [03:41<03:16, 28.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Wall Of Death - Femme': 59779 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  62%|██████▎   | 10/16 [03:57<02:24, 24.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Young Griffo - Blood To Bone': 63645 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  69%|██████▉   | 11/16 [04:17<01:55, 23.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Scarlet Brand - Les Fleurs Du Mal': 75909 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  75%|███████▌  | 12/16 [04:34<01:24, 21.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'Tim Taler - Stalker': 59454 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  81%|████████▏ | 13/16 [04:39<00:48, 16.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The So So Glos - Emergency': 41750 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  88%|████████▊ | 14/16 [05:29<00:53, 26.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Districts - Vermont': 57038 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing dataset:  94%|█████████▍| 15/16 [05:47<00:23, 23.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Long Wait - Back Home To Blue': 65162 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing dataset: 100%|██████████| 16/16 [06:01<00:00, 22.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 'The Wrong'Uns - Rothko': 50584 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train = load_audio_data(train_dataset_path)\n",
        "x_test, y_test = load_audio_data(test_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9tu-Pd7Utl5",
        "outputId": "0052f2fc-aa03-4be2-e870-1ed3a0cbf1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Save the datasets\n",
        "np.save('x_train.npy', x_train)\n",
        "np.save('y_train_bass.npy', y_train)\n",
        "np.save('x_test.npy', x_test)\n",
        "np.save('y_test_bass.npy', y_test)\n",
        "\n",
        "print(\"Data saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwxSTNfpUtl6",
        "outputId": "9c4aae68-ba14-4034-97e5-4e916eca84f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "x_train = np.load('x_train.npy')\n",
        "y_train = np.load('y_train_bass.npy')\n",
        "x_test = np.load('x_test.npy')\n",
        "y_test = np.load('y_test_bass.npy')\n",
        "\n",
        "print(\"Data loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXi4K6zEUtl7"
      },
      "outputs": [],
      "source": [
        "x_train = np.expand_dims(x_train, -1)\n",
        "y_train = np.expand_dims(y_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "y_test = np.expand_dims(y_test, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU4x3hNeUtl7",
        "outputId": "78fc8291-8ac3-41f6-eb94-9e824aa15b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 64, 1)]              0         []                            \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)             (None, 64, 16)               96        ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 64, 16)               64        ['conv1d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)     (None, 64, 16)               0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)           (None, 64, 32)               2592      ['leaky_re_lu[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 64, 32)               128       ['conv1d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 64, 32)               0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)           (None, 64, 64)               10304     ['leaky_re_lu_1[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 64, 64)               256       ['conv1d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 64, 64)               0         ['batch_normalization_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)           (None, 64, 128)              41088     ['leaky_re_lu_2[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 64, 128)              512       ['conv1d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 64, 128)              0         ['batch_normalization_3[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv1d_transpose (Conv1DTr  (None, 64, 64)               41024     ['leaky_re_lu_3[0][0]']       \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 64, 128)              0         ['conv1d_transpose[0][0]',    \n",
            "                                                                     'leaky_re_lu_2[0][0]']       \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                (None, 64, 128)              0         ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " conv1d_transpose_1 (Conv1D  (None, 64, 32)               20512     ['re_lu[0][0]']               \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 64, 64)               0         ['conv1d_transpose_1[0][0]',  \n",
            " )                                                                   'leaky_re_lu_1[0][0]']       \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)              (None, 64, 64)               0         ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_transpose_2 (Conv1D  (None, 64, 16)               5136      ['re_lu_1[0][0]']             \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 64, 32)               0         ['conv1d_transpose_2[0][0]',  \n",
            " )                                                                   'leaky_re_lu[0][0]']         \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)              (None, 64, 32)               0         ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " conv1d_transpose_3 (Conv1D  (None, 64, 1)                161       ['re_lu_2[0][0]']             \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 64, 1)                0         ['input_1[0][0]',             \n",
            "                                                                     'conv1d_transpose_3[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 121873 (476.07 KB)\n",
            "Trainable params: 121393 (474.19 KB)\n",
            "Non-trainable params: 480 (1.88 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_model(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "\n",
        "    conv1 = Conv1D(filters=16, kernel_size=5, strides=1, padding='same')(input_layer)\n",
        "    batch1 = BatchNormalization()(conv1)\n",
        "    activ1 = LeakyReLU(alpha=0.2)(batch1)\n",
        "\n",
        "    conv2 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(activ1)\n",
        "    batch2 = BatchNormalization()(conv2)\n",
        "    activ2 = LeakyReLU(alpha=0.2)(batch2)\n",
        "\n",
        "    conv3 = Conv1D(filters=64, kernel_size=5, strides=1, padding='same')(activ2)\n",
        "    batch3 = BatchNormalization()(conv3)\n",
        "    activ3 = LeakyReLU(alpha=0.2)(batch3)\n",
        "\n",
        "    conv4 = Conv1D(filters=128, kernel_size=5, strides=1, padding='same')(activ3)\n",
        "    batch4 = BatchNormalization()(conv4)\n",
        "    activ4 = LeakyReLU(alpha=0.2)(batch4)\n",
        "\n",
        "\n",
        "    up3 = Conv1DTranspose(filters=64, kernel_size=5, strides=1, padding='same')(activ4)\n",
        "    conc3 = Concatenate()([up3, activ3])\n",
        "    uactiv3 = ReLU()(conc3)\n",
        "\n",
        "    up2 = Conv1DTranspose(filters=32, kernel_size=5, strides=1, padding='same')(uactiv3)\n",
        "    conc2 = Concatenate()([up2, activ2])\n",
        "    uactiv2 = ReLU()(conc2)\n",
        "\n",
        "    up1 = Conv1DTranspose(filters=16, kernel_size=5, strides=1, padding='same')(uactiv2)\n",
        "    conc1 = Concatenate()([up1, activ1])\n",
        "    uactiv1 = ReLU()(conc1)\n",
        "\n",
        "    # Output\n",
        "    mask = Conv1DTranspose(filters=1, kernel_size=5, strides=1, padding='same')(uactiv1)\n",
        "    output_layer = Multiply()([input_layer, mask])\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Build and compile the model\n",
        "model = build_model((64, 1))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"mae\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqMrJJw2Utl8"
      },
      "outputs": [],
      "source": [
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-6)\n",
        "checkpoint = ModelCheckpoint('best_model_bass.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_test, y_test),\n",
        "    epochs=500,\n",
        "    batch_size=32,\n",
        "    verbose=0,\n",
        "    callbacks=[lr_reduction, checkpoint, TqdmCallback()]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBNZSUVjUtl9"
      },
      "outputs": [],
      "source": [
        "model.save('unet_source_separation_bass.h5')\n",
        "print(\"Model saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
